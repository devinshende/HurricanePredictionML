{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow.keras as keras\n",
    "import h5py\n",
    "import PIL.Image\n",
    "import os, shutil\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import models\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras import optimizers\n",
    "from tensorflow.keras.preprocessing import image\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.layers import LeakyReLU\n",
    "from tensorflow.keras.regularizers import l2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = load_model('tomnod_1_50epochs.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "original_dataset_dir = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_damage_dir = original_dataset_dir + '/train/damage'\n",
    "validation_damage_dir = original_dataset_dir + '/validation/damage'\n",
    "test_damage_dir = original_dataset_dir + '/test/damage'\n",
    "\n",
    "train_nodamage_dir = original_dataset_dir + '/train/no_damage'\n",
    "validation_nodamage_dir = original_dataset_dir + '/validation/no_damage'\n",
    "test_nodamage_dir = original_dataset_dir + '/test/no_damage'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('total training damage images: ',len(os.listdir(train_damage_dir)))\n",
    "print('total validation damage images: ',len(os.listdir(validation_damage_dir)))\n",
    "print('total test damage images: ',len(os.listdir(test_damage_dir)))\n",
    "\n",
    "print('total training no damage images: ',len(os.listdir(train_nodamage_dir)))\n",
    "print('total validation no damage images: ',len(os.listdir(validation_nodamage_dir)))\n",
    "print('total test no damage images: ',len(os.listdir(test_nodamage_dir)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras import layers\n",
    "from keras import models\n",
    "model = models.Sequential()\n",
    "model.add(layers.Conv2D(32,(3,3), activation = 'relu', input_shape = (150,150,3)))\n",
    "model.add(layers.MaxPooling2D((2,2)))\n",
    "model.add(layers.Conv2D(64,(3,3), activation = 'relu'))\n",
    "model.add(layers.MaxPooling2D((2,2)))\n",
    "model.add(layers.Conv2D(128,(3,3), activation = 'relu'))\n",
    "model.add(layers.MaxPooling2D((2,2)))\n",
    "model.add(layers.Conv2D(128,(3,3), activation = 'relu'))\n",
    "model.add(layers.MaxPooling2D((2,2)))\n",
    "model.add(layers.Flatten())\n",
    "model.add(layers.Dense(512,activation = 'relu'))\n",
    "model.add(layers.Dense(1, activation = 'sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#compile the model with RMSprob with learning rate\n",
    "from keras import optimizers\n",
    "model.compile(loss = 'binary_crossentropy', optimizer = optimizers.RMSprop(lr=1e-4), metrics = ['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#process the jpeg image\n",
    "#create an image generator\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "#normalize the image pixel value to be between 0 and 1\n",
    "train_datagen = ImageDataGenerator(rescale=1./255)\n",
    "test_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "                    '/home/ubuntu/train', \n",
    "                    target_size = (150,150),\n",
    "                    batch_size = 20,\n",
    "                    class_mode = 'binary')\n",
    "validation_generator = test_datagen.flow_from_directory(\n",
    "                    '/home/ubuntu/validation',\n",
    "                    target_size = (150,150),\n",
    "                    batch_size = 20,\n",
    "                    class_mode = 'binary')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fit the model from image generator\n",
    "history = model.fit_generator(\n",
    "            train_generator,\n",
    "            steps_per_epoch=100,\n",
    "            epochs=50,\n",
    "            validation_data=validation_generator,\n",
    "            validation_steps=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#it is generally a good practice to save the model after training\n",
    "model.save('tomnod_1.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot loss and accuracy for detection overfitting (30 epochs)\n",
    "import matplotlib.pyplot as plt\n",
    "acc = history.history['acc']\n",
    "val_acc = history.history['val_acc']\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "epochs = range(1,len(acc)+1)\n",
    "plt.plot(epochs, acc, 'bo', label = 'Training acc')\n",
    "plt.plot(epochs, val_acc, 'r', label = 'Validation acc')\n",
    "plt.title('Training and validation accuracy with ReLU')\n",
    "plt.legend()\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(epochs, loss, 'bo', label = 'Training loss')\n",
    "plt.plot(epochs, val_loss, 'r', label = 'Validation loss')\n",
    "plt.title('Training and validation loss with ReLU')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#it is generally a good practice to save the model after training\n",
    "model.save('tomnod_1_50epochs.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot loss and accuracy for detection overfitting (50 epochs)\n",
    "import matplotlib.pyplot as plt\n",
    "acc = history.history['acc']\n",
    "val_acc = history.history['val_acc']\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "epochs = range(1,len(acc)+1)\n",
    "plt.plot(epochs, acc, 'bo', label = 'Training acc')\n",
    "plt.plot(epochs, val_acc, 'b', label = 'Validation acc')\n",
    "plt.title('Training and validation accuracy with ReLU')\n",
    "plt.legend()\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(epochs, loss, 'bo', label = 'Training loss')\n",
    "plt.plot(epochs, val_loss, 'b', label = 'Validation loss')\n",
    "plt.title('Training and validation loss with ReLU')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#the accuracy stalls at about 95.5% and we start seeing overfitting\n",
    "#we perform data augmentation\n",
    "#to fight overfitting, we add a Dropout layer right before \n",
    "#the densely connected layer\n",
    "from keras import models\n",
    "from keras import layers\n",
    "from keras import optimizers\n",
    "model = models.Sequential()\n",
    "model.add(layers.Conv2D(32,(3,3), activation = 'relu', input_shape = (150,150,3)))\n",
    "model.add(layers.MaxPooling2D((2,2)))\n",
    "model.add(layers.Conv2D(64,(3,3), activation = 'relu'))\n",
    "model.add(layers.MaxPooling2D((2,2)))\n",
    "model.add(layers.Conv2D(128,(3,3), activation = 'relu'))\n",
    "model.add(layers.MaxPooling2D((2,2)))\n",
    "model.add(layers.Conv2D(128,(3,3), activation = 'relu'))\n",
    "model.add(layers.MaxPooling2D((2,2)))\n",
    "model.add(layers.Flatten())\n",
    "model.add(layers.Dropout(0.5))\n",
    "model.add(layers.Dense(512,activation = 'relu'))\n",
    "model.add(layers.Dense(1, activation = 'sigmoid'))\n",
    "\n",
    "model.compile(loss = 'binary_crossentropy',\n",
    "                optimizer = optimizers.RMSprop(lr = 1e-4),\n",
    "                 metrics = ['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train using data augmentation and dropout\n",
    "train_datagen = ImageDataGenerator(\n",
    "                    rescale = 1./255,\n",
    "                    rotation_range = 40,\n",
    "                    width_shift_range = 0.2,\n",
    "                    height_shift_range = 0.2,\n",
    "                    shear_range = 0.2,\n",
    "                    zoom_range = 0.2,\n",
    "                    horizontal_flip = True)\n",
    "test_datagen = ImageDataGenerator(rescale = 1./255) #validation data should not be augmented\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "                    '/home/ubuntu/train', \n",
    "                    target_size = (150,150),\n",
    "                    batch_size = 32,\n",
    "                    class_mode = 'binary')\n",
    "validation_generator = test_datagen.flow_from_directory(\n",
    "                    '/home/ubuntu/validation', \n",
    "                    target_size = (150,150),\n",
    "                    batch_size = 32,\n",
    "                    class_mode = 'binary')\n",
    "#train the model with fit_generator\n",
    "history = model.fit_generator(\n",
    "            train_generator,\n",
    "            steps_per_epoch = 100,\n",
    "            epochs = 100,\n",
    "            validation_data = validation_generator,\n",
    "            validation_steps = 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#it is generally a good practice to save the model after training\n",
    "model.save('tomnod_2_100epochs.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot loss and accuracy for detection overfitting\n",
    "import matplotlib.pyplot as plt\n",
    "acc = history.history['acc']\n",
    "val_acc = history.history['val_acc']\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "epochs = range(1,len(acc)+1)\n",
    "plt.plot(epochs, acc, 'bo', label = 'Training acc')\n",
    "plt.plot(epochs, val_acc, 'r', label = 'Validation acc')\n",
    "plt.title('Training and validation accuracy with data aug and dropout')\n",
    "plt.legend()\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(epochs, loss, 'bo', label = 'Training loss')\n",
    "plt.plot(epochs, val_loss, 'r', label = 'Validation loss')\n",
    "plt.title('Training and validation loss with data aug and dropout')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#validation accuracy can go up to 97.44%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#try again with Adam optimizer\n",
    "model = models.Sequential()\n",
    "model.add(layers.Conv2D(32,(3,3), activation = 'relu', input_shape = (150,150,3)))\n",
    "model.add(layers.MaxPooling2D((2,2)))\n",
    "model.add(layers.Conv2D(64,(3,3), activation = 'relu'))\n",
    "model.add(layers.MaxPooling2D((2,2)))\n",
    "model.add(layers.Conv2D(128,(3,3), activation = 'relu'))\n",
    "model.add(layers.MaxPooling2D((2,2)))\n",
    "model.add(layers.Conv2D(128,(3,3), activation = 'relu'))\n",
    "model.add(layers.MaxPooling2D((2,2)))\n",
    "model.add(layers.Flatten())\n",
    "model.add(layers.Dropout(0.5))\n",
    "model.add(layers.Dense(512,activation = 'relu'))\n",
    "model.add(layers.Dense(1, activation = 'sigmoid'))\n",
    "\n",
    "model.compile(loss = 'binary_crossentropy',\n",
    "                optimizer = optimizers.Adam(),\n",
    "                 metrics = ['acc'])\n",
    "\n",
    "#train using data augmentation and dropout\n",
    "train_datagen = ImageDataGenerator(\n",
    "                    rescale = 1./255,\n",
    "                    rotation_range = 40,\n",
    "                    width_shift_range = 0.2,\n",
    "                    height_shift_range = 0.2,\n",
    "                    shear_range = 0.2,\n",
    "                    zoom_range = 0.2,\n",
    "                    horizontal_flip = True)\n",
    "test_datagen = ImageDataGenerator(rescale = 1./255) #validation data should not be augmented\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "                    '/home/ubuntu/train', \n",
    "                    target_size = (150,150),\n",
    "                    batch_size = 32,\n",
    "                    class_mode = 'binary')\n",
    "validation_generator = test_datagen.flow_from_directory(\n",
    "                    '/home/ubuntu/validation', \n",
    "                    target_size = (150,150),\n",
    "                    batch_size = 32,\n",
    "                    class_mode = 'binary')\n",
    "#train the model with fit_generator\n",
    "history = model.fit_generator(\n",
    "            train_generator,\n",
    "            steps_per_epoch = 100,\n",
    "            epochs = 100,\n",
    "            validation_data = validation_generator,\n",
    "            validation_steps = 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.save('tomnod_2_100epochs_Adam.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot loss and accuracy for detection overfitting\n",
    "import matplotlib.pyplot as plt\n",
    "acc = history.history['acc']\n",
    "val_acc = history.history['val_acc']\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "epochs = range(1,len(acc)+1)\n",
    "plt.plot(epochs, acc, 'bo', label = 'Training acc')\n",
    "plt.plot(epochs, val_acc, 'r', label = 'Validation acc')\n",
    "plt.title('Training and validation accuracy with data aug and dropout (Adam)')\n",
    "plt.legend()\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(epochs, loss, 'bo', label = 'Training loss')\n",
    "plt.plot(epochs, val_loss, 'r', label = 'Validation loss')\n",
    "plt.title('Training and validation loss with data aug and dropout (Adam)')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#to visualize the intermediate activation\n",
    "from keras.models import load_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model1 = load_model('tomnod_1.h5')\n",
    "model1.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_path = '/home/ubuntu/damage/-96.960704_28.783292.jpeg'\n",
    "from keras.preprocessing import image\n",
    "import numpy as np\n",
    "\n",
    "img = image.load_img(img_path, target_size = (150,150))\n",
    "img_tensor = image.img_to_array(img)\n",
    "img_tensor = np.expand_dims(img_tensor, axis = 0)\n",
    "img_tensor /= 255.\n",
    "\n",
    "print(img_tensor.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(img_tensor[0])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras import models\n",
    "layer_outputs = [layer.output for layer in model.layers[:8]]\n",
    "activation_model = models.Model(inputs = model.input, outputs = layer_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "activations = activation_model.predict(img_tensor)\n",
    "first_layer_activation = activations[0]\n",
    "plt.matshow(first_layer_activation[0,:,:,4],cmap = 'viridis')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "second_layer_activation = activations[1]\n",
    "plt.matshow(second_layer_activation[0,:,:,4],cmap = 'viridis')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "third_layer_activation = activations[2]\n",
    "plt.matshow(third_layer_activation[0,:,:,4],cmap = 'viridis')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_names =[]\n",
    "for layer in model.layers[:8]:\n",
    "    layer_names.append(layer.name)\n",
    "images_per_row = 16\n",
    "\n",
    "for layer_name, layer_activation in zip(layer_names, activations):\n",
    "    n_features = layer_activation.shape[-1]\n",
    "    size = layer_activation.shape[1]\n",
    "    n_cols = n_features//images_per_row\n",
    "    display_grid = np.zeros((size*n_cols, images_per_row*size))\n",
    "    for col in range(n_cols):\n",
    "        for row in range(images_per_row):\n",
    "            channel_image = layer_activation[0,:,:,col*images_per_row+row]\n",
    "            channel_image -= channel_image.mean()\n",
    "            channel_image /= channel_image.std()\n",
    "            channel_image *= 64\n",
    "            channel_image += 128\n",
    "            channel_image = np.clip(channel_image, 0, 255).astype('uint8')\n",
    "            display_grid[col*size:(col+1)*size,row*size:(row+1)*size] = channel_image\n",
    "    scale = 1./size\n",
    "    plt.figure(figsize=(scale*display_grid.shape[1],\n",
    "                        scale*display_grid.shape[0]))\n",
    "    plt.title(layer_name)\n",
    "    plt.grid(False)\n",
    "    plt.imshow(display_grid, aspect = 'auto', cmap = 'viridis')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.savefig('informationFlow-relu.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#let's build a model with leaky ReLU\n",
    "from keras.layers import LeakyReLU\n",
    "from keras.regularizers import l2\n",
    "model_l = models.Sequential()\n",
    "model_l.add(layers.Conv2D(32,(3,3), input_shape = (150,150,3)))\n",
    "model_l.add(LeakyReLU(alpha=0.1))\n",
    "model_l.add(layers.MaxPooling2D((2,2)))\n",
    "#model_l.add(Dropout(0.25))\n",
    "\n",
    "model_l.add(layers.Conv2D(64,(3,3)))\n",
    "model_l.add(LeakyReLU(alpha=0.1))\n",
    "model_l.add(layers.MaxPooling2D((2,2)))\n",
    "#model_l.add(Dropout(0.25))\n",
    "\n",
    "model_l.add(layers.Conv2D(128,(3,3)))\n",
    "model_l.add(LeakyReLU(alpha=0.1))\n",
    "model_l.add(layers.MaxPooling2D((2,2)))\n",
    "#model_l.add(Dropout(0.25))\n",
    "\n",
    "model_l.add(layers.Conv2D(128,(3,3)))\n",
    "model_l.add(LeakyReLU(alpha=0.1))\n",
    "model_l.add(layers.MaxPooling2D((2,2)))\n",
    "#model_l.add(Dropout(0.25))\n",
    "\n",
    "model_l.add(layers.Flatten())\n",
    "model_l.add(layers.Dense(512,W_regularizer = l2(1e-6)))\n",
    "model_l.add(LeakyReLU(alpha=0.1))\n",
    "model_l.add(layers.Dense(1, activation = 'sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_l.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#compile the model with RMSprob with learning rate\n",
    "from keras import optimizers\n",
    "model_l.compile(loss = 'binary_crossentropy', optimizer = optimizers.RMSprop(lr=1e-4), metrics = ['acc'])\n",
    "\n",
    "#process the jpeg image\n",
    "#create an image generator\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "train_datagen = ImageDataGenerator(rescale=1./255)\n",
    "test_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "                    '/home/ubuntu/train', \n",
    "                    target_size = (150,150),\n",
    "                    batch_size = 20,\n",
    "                    class_mode = 'binary')\n",
    "validation_generator = test_datagen.flow_from_directory(\n",
    "                    '/home/ubuntu/validation',\n",
    "                    target_size = (150,150),\n",
    "                    batch_size = 20,\n",
    "                    class_mode = 'binary')\n",
    "\n",
    "#fit the model from image generator\n",
    "history = model_l.fit_generator(\n",
    "            train_generator,\n",
    "            steps_per_epoch=100,\n",
    "            epochs=30,\n",
    "            validation_data=validation_generator,\n",
    "            validation_steps=50)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#it is generally a good practice to save the model after training\n",
    "model_l.save('tomnod_leaky_30epochs.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot loss and accuracy for detection overfitting\n",
    "import matplotlib.pyplot as plt\n",
    "acc = history.history['acc']\n",
    "val_acc = history.history['val_acc']\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "epochs = range(1,len(acc)+1)\n",
    "plt.plot(epochs, acc, 'bo', label = 'Training acc')\n",
    "plt.plot(epochs, val_acc, 'r', label = 'Validation acc')\n",
    "plt.title('Training and validation accuracy with leaky ReLU')\n",
    "plt.legend()\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(epochs, loss, 'bo', label = 'Training loss')\n",
    "plt.plot(epochs, val_loss, 'r', label = 'Validation loss')\n",
    "plt.title('Training and validation loss with leaky ReLU')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = load_model('tomnod_leaky_30epochs.h5')\n",
    "model.summary()\n",
    "img_path = '/home/ubuntu/damage/-96.960704_28.783292.jpeg'\n",
    "from keras.preprocessing import image\n",
    "import numpy as np\n",
    "\n",
    "img = image.load_img(img_path, target_size = (150,150))\n",
    "img_tensor = image.img_to_array(img)\n",
    "img_tensor = np.expand_dims(img_tensor, axis = 0)\n",
    "img_tensor /= 255.\n",
    "\n",
    "print(img_tensor.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_names =[]\n",
    "for layer in model.layers[:8]:\n",
    "    layer_names.append(layer.name)\n",
    "images_per_row = 16\n",
    "\n",
    "for layer_name, layer_activation in zip(layer_names, activations):\n",
    "    n_features = layer_activation.shape[-1]\n",
    "    size = layer_activation.shape[1]\n",
    "    n_cols = n_features//images_per_row\n",
    "    display_grid = np.zeros((size*n_cols, images_per_row*size))\n",
    "    for col in range(n_cols):\n",
    "        for row in range(images_per_row):\n",
    "            channel_image = layer_activation[0,:,:,col*images_per_row+row]\n",
    "            channel_image -= channel_image.mean()\n",
    "            channel_image /= channel_image.std()\n",
    "            channel_image *= 64\n",
    "            channel_image += 128\n",
    "            channel_image = np.clip(channel_image, 0, 255).astype('uint8')\n",
    "            display_grid[col*size:(col+1)*size,row*size:(row+1)*size] = channel_image\n",
    "    scale = 1./size\n",
    "    plt.figure(figsize=(scale*display_grid.shape[1],\n",
    "                        scale*display_grid.shape[0]))\n",
    "    plt.title(layer_name)\n",
    "    plt.grid(False)\n",
    "    plt.imshow(display_grid, aspect = 'auto', cmap = 'viridis')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.savefig('informationFlow-leaky.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#try transfer learning\n",
    "#using feature extraction from ImageNet VGG16\n",
    "from keras.applications import VGG16\n",
    "conv_base = VGG16(weights = 'imagenet',\n",
    "                    include_top = False, #no dense layers\n",
    "                    input_shape = (150,150,3))\n",
    "conv_base.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Method 1: Fast feature extraction without data augmentation\n",
    "#Run the conv_base on the dataset and save as Numpy array on disk\n",
    "#Then build the dense layer on this\n",
    "#This is faster to run, but we cannot augment the data\n",
    "import os\n",
    "import numpy as np\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "train_dir = original_dataset_dir + '/train'\n",
    "validation_dir = original_dataset_dir + '/validation'\n",
    "test_dir = original_dataset_dir + '/test'\n",
    "\n",
    "datagen = ImageDataGenerator(rescale=1./255)\n",
    "batch_size = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def extract_features(directory, sample_count):\n",
    "    features = np.zeros(shape=(sample_count,4,4,512))\n",
    "    labels = np.zeros(shape = (sample_count))\n",
    "    generator = datagen.flow_from_directory(\n",
    "                    directory, \n",
    "                    target_size = (150,150),\n",
    "                    batch_size = batch_size,\n",
    "                    class_mode = 'binary')\n",
    "    i = 0\n",
    "    for inputs_batch, labels_batch in generator:\n",
    "        features_batch = conv_base.predict(inputs_batch)\n",
    "        features[i * batch_size: (i + 1) * batch_size] = features_batch\n",
    "        labels[i * batch_size: (i + 1) * batch_size] = labels_batch\n",
    "        i += 1\n",
    "        if i * batch_size >= sample_count: #generators yield data indefinitely\n",
    "            break                          #have to break after we have seen every image once\n",
    "    return features, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_features, train_labels = extract_features(train_dir, 10000)\n",
    "validation_features, validation_labels = extract_features(validation_dir, 2000)\n",
    "test_features, test_labels = extract_features(test_dir, 2000)\n",
    "#the extracted features are of shape (sample_count, 4, 4, 512), we must flatten them to (sample_count, 8192)\n",
    "train_features = np.reshape(train_features, (10000, 4*4*512))\n",
    "validation_features = np.reshape(validation_features, (2000, 4*4*512))\n",
    "test_features = np.reshape(test_features, (2000, 4*4*512))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define the densely connected layer\n",
    "from keras import models\n",
    "from keras import layers\n",
    "from keras import optimizers\n",
    "\n",
    "model = models.Sequential()\n",
    "model.add(layers.Dense(256, activation = 'relu', input_dim = 4*4*512))\n",
    "model.add(layers.Dropout(0.5))\n",
    "model.add(layers.Dense(1, activation = 'sigmoid'))\n",
    "\n",
    "model.compile(optimizer = optimizers.RMSprop(lr = 2e-5),\n",
    "              loss = 'binary_crossentropy',\n",
    "              metrics = ['acc'])\n",
    "\n",
    "history = model.fit(train_features, train_labels, \n",
    "                    epochs = 30,\n",
    "                    batch_size = 20,\n",
    "                    validation_data = (validation_features, validation_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#save the model with pretrained features in Numpy array (no data augmentation)\n",
    "model.save('tomnod_transfer_1.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot loss and accuracy for detection overfitting\n",
    "import matplotlib.pyplot as plt\n",
    "acc = history.history['acc']\n",
    "val_acc = history.history['val_acc']\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "epochs = range(1,len(acc)+1)\n",
    "plt.plot(epochs, acc, 'bo', label = 'Training acc')\n",
    "plt.plot(epochs, val_acc, 'r', label = 'Validation acc')\n",
    "plt.title('Training and validation accuracy with pretrained features as Numpy array')\n",
    "plt.legend()\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(epochs, loss, 'bo', label = 'Training loss')\n",
    "plt.plot(epochs, val_loss, 'r', label = 'Validation loss')\n",
    "plt.title('Training and validation loss with pretrained features as Numpy array')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# we see that the feature extraction is good but we start overfitting almost in the first few epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#feature extraction with data augmentation and dropout\n",
    "from keras import models\n",
    "from keras import layers\n",
    "model = models.Sequential()\n",
    "model.add(conv_base)\n",
    "model.add(layers.Flatten())\n",
    "model.add(layers.Dense(256,activation='relu'))\n",
    "model.add(layers.Dense(1,activation = 'sigmoid'))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv_base.trainable = False #freeze the convolutional base network\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras import optimizers\n",
    "\n",
    "train_datagen = ImageDataGenerator(rescale=1./255,\n",
    "                                   rotation_range = 40,\n",
    "                                   width_shift_range = 0.2,\n",
    "                                   height_shift_range = 0.2,\n",
    "                                   shear_range = 0.2,\n",
    "                                   zoom_range = 0.2,\n",
    "                                   horizontal_flip = True,\n",
    "                                   fill_mode = 'nearest')\n",
    "test_datagen = ImageDataGenerator(rescale = 1./255)\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "                    '/home/ubuntu/train',\n",
    "                    target_size=(150,150),\n",
    "                    batch_size = 20,\n",
    "                    class_mode = 'binary')\n",
    "\n",
    "validation_generator = test_datagen.flow_from_directory(\n",
    "                    '/home/ubuntu/validation',\n",
    "                    target_size=(150,150),\n",
    "                    batch_size = 20,\n",
    "                    class_mode = 'binary')\n",
    "\n",
    "model.compile(loss = 'binary_crossentropy',\n",
    "              optimizer = optimizers.RMSprop(lr = 2e-5),\n",
    "              metrics = ['acc'])\n",
    "\n",
    "history = model.fit_generator(\n",
    "            train_generator,\n",
    "            steps_per_epoch = 100,\n",
    "            epochs = 30,\n",
    "            validation_data = validation_generator,\n",
    "            validation_steps = 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#save the model with transfer learning and data augmentation\n",
    "model.save('tomnod_transfer_dataAugment.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot loss and accuracy for detection overfitting\n",
    "import matplotlib.pyplot as plt\n",
    "acc = history.history['acc']\n",
    "val_acc = history.history['val_acc']\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "epochs = range(1,len(acc)+1)\n",
    "plt.plot(epochs, acc, 'bo', label = 'Training acc')\n",
    "plt.plot(epochs, val_acc, 'r', label = 'Validation acc')\n",
    "plt.title('Training and validation accuracy with pretrained features and data augmentation')\n",
    "plt.legend()\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(epochs, loss, 'bo', label = 'Training loss')\n",
    "plt.plot(epochs, val_loss, 'r', label = 'Validation loss')\n",
    "plt.title('Training and validation loss with pretrained features and data augmentation')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#we still need a final, grand model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#temp\n",
    "#let's build a model with leaky ReLU\n",
    "from keras.layers import LeakyReLU\n",
    "from keras.regularizers import l2\n",
    "model = models.Sequential()\n",
    "model.add(layers.Conv2D(32,(3,3), input_shape = (150,150,3)))\n",
    "model.add(LeakyReLU(alpha=0.1))\n",
    "model.add(layers.MaxPooling2D((2,2)))\n",
    "model.add(layers.Dropout(0.25))\n",
    "\n",
    "model.add(layers.Conv2D(64,(3,3)))\n",
    "model.add(LeakyReLU(alpha=0.1))\n",
    "model.add(layers.MaxPooling2D((2,2)))\n",
    "model.add(layers.Dropout(0.25))\n",
    "\n",
    "model.add(layers.Conv2D(128,(3,3)))\n",
    "model.add(LeakyReLU(alpha=0.1))\n",
    "model.add(layers.MaxPooling2D((2,2)))\n",
    "model.add(layers.Dropout(0.25))\n",
    "\n",
    "model.add(layers.Conv2D(128,(3,3)))\n",
    "model.add(LeakyReLU(alpha=0.1))\n",
    "model.add(layers.MaxPooling2D((2,2)))\n",
    "model.add(layers.Dropout(0.25))\n",
    "\n",
    "model.add(layers.Flatten())\n",
    "model.add(layers.Dropout(0.5))\n",
    "model.add(layers.Dense(512,W_regularizer = l2(1e-6)))\n",
    "model.add(LeakyReLU(alpha=0.1))\n",
    "model.add(layers.Dense(1, activation = 'sigmoid'))\n",
    "#compile the model with RMSprob with learning rate\n",
    "from keras import optimizers\n",
    "model.compile(loss = 'binary_crossentropy', optimizer = optimizers.RMSprop(lr=1e-4), metrics = ['acc'])\n",
    "\n",
    "#process the jpeg image\n",
    "#create an image generator\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "#train using data augmentation and dropout\n",
    "train_datagen = ImageDataGenerator(\n",
    "                    rescale = 1./255,\n",
    "                    rotation_range = 40,\n",
    "                    width_shift_range = 0.2,\n",
    "                    height_shift_range = 0.2,\n",
    "                    shear_range = 0.2,\n",
    "                    zoom_range = 0.2,\n",
    "                    horizontal_flip = True)\n",
    "test_datagen = ImageDataGenerator(rescale = 1./255) #validation data should not be augmented\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "                    '/home/ubuntu/train', \n",
    "                    target_size = (150,150),\n",
    "                    batch_size = 32,\n",
    "                    class_mode = 'binary')\n",
    "validation_generator = test_datagen.flow_from_directory(\n",
    "                    '/home/ubuntu/validation', \n",
    "                    target_size = (150,150),\n",
    "                    batch_size = 32,\n",
    "                    class_mode = 'binary')\n",
    "\n",
    "#fit the model from image generator\n",
    "history = model.fit_generator(\n",
    "            train_generator,\n",
    "            steps_per_epoch=100,\n",
    "            epochs=100,\n",
    "            validation_data=validation_generator,\n",
    "            validation_steps=50)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#save the champion model (leaky, full dropout, L2 regularization, data aug)\n",
    "model.save('tomnod_everything.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot loss and accuracy for detection overfitting\n",
    "import matplotlib.pyplot as plt\n",
    "acc = history.history['acc']\n",
    "val_acc = history.history['val_acc']\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "epochs = range(1,len(acc)+1)\n",
    "plt.plot(epochs, acc, 'bo', label = 'Training acc')\n",
    "plt.plot(epochs, val_acc, 'r', label = 'Validation acc')\n",
    "plt.title('Training and validation accuracy with leaky, full dropout, L2 regularization, data aug')\n",
    "plt.legend()\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(epochs, loss, 'bo', label = 'Training loss')\n",
    "plt.plot(epochs, val_loss, 'r', label = 'Validation loss')\n",
    "plt.title('Training and validation loss with leaky, full dropout, L2 regularization, data aug')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#write the code to compute test set accuracy\n",
    "#process the jpeg image\n",
    "#create an image generator for the test set\n",
    "test_datagen = ImageDataGenerator(rescale=1./255)\n",
    "test_generator = test_datagen.flow_from_directory(\n",
    "                    '/home/ubuntu/test_another', #change this for balanced/unbalanced test set \n",
    "                    target_size = (150,150),\n",
    "                    batch_size = 20,\n",
    "                    class_mode = 'binary')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# uncomment to get the respective model \n",
    "# model = load_model('tomnod_1.h5')\n",
    "# model = load_model('tomnod_1_50epochs.h5')\n",
    "# model = load_model('tomnod_2_100epochs.h5')\n",
    "# model = load_model('tomnod_leaky_30epochs.h5')\n",
    "# model = load_model('tomnod_transfer_dataAugment.h5')\n",
    "# model = load_model('tomnod_everything.h5')\n",
    "# model = load_model('tomnod_everything_relu.h5')\n",
    "# model = load_model('tomnod_everything_relu_Adam.h5')\n",
    "# model = load_model('tomnod_2_100epochs_Adam.h5')\n",
    "\n",
    "test_results = model.evaluate_generator(test_generator)\n",
    "test_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the transfer learning model using Numpy array is different from the rest\n",
    "model = load_model('tomnod_transfer_1.h5')\n",
    "test_results = model.evaluate(test_features,test_labels)\n",
    "test_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a logistic regression using Numpy array features (after passing through convultional based)\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "logisticRegr = LogisticRegression(C = 1)\n",
    "logisticRegr.fit(train_features,train_labels)\n",
    "#balanced test set \n",
    "score1 = logisticRegr.score(validation_features, validation_labels)\n",
    "print(\"Validation acc = \", score1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#unbalanced test set \n",
    "score2 = logisticRegr.score(test_features, test_labels)\n",
    "print(\"Test acc = \", score2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = load_model('tomnod_2_100epochs.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve\n",
    "test_datagen = ImageDataGenerator(rescale=1./255)\n",
    "test_generator = test_datagen.flow_from_directory(\n",
    "                    '/home/ubuntu/test_another', #change this for balanced/unbalanced test set \n",
    "                    target_size = (150,150),\n",
    "                    batch_size = 20,\n",
    "                    class_mode = 'binary')\n",
    "y_pred = []\n",
    "y_label = []\n",
    "for i in range(450):\n",
    "    x,y = test_generator.next()\n",
    "    temp_y = model.predict(x)\n",
    "    y.tolist()\n",
    "    temp_y.tolist()\n",
    "    y_pred.extend(temp_y)\n",
    "    #print(temp_y)\n",
    "    #print(y)\n",
    "    #print('')\n",
    "    y_label.extend(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(y_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fpr_keras, tpr_keras, thresholds_keras = roc_curve(y_label, y_pred,pos_label=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import auc\n",
    "auc_keras = auc(fpr_keras, tpr_keras)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(auc_keras)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(1)\n",
    "plt.plot([0, 1], [0, 1], 'k--')\n",
    "plt.plot(fpr_keras, tpr_keras, label='AUC area = {:.3f}'.format(auc_keras))\n",
    "#plt.plot(fpr_rf, tpr_rf, label='RF (area = {:.3f})'.format(auc_rf))\n",
    "plt.xlabel('False positive rate')\n",
    "plt.ylabel('True positive rate')\n",
    "plt.title('ROC curve of the best model with unbalanced test set')\n",
    "plt.legend(loc='best')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#see which image is misclassified\n",
    "test_datagen = ImageDataGenerator(rescale=1./255)\n",
    "test_generator = test_datagen.flow_from_directory(\n",
    "                    '/home/ubuntu/test', #change this for balanced/unbalanced test set \n",
    "                    target_size = (150,150),\n",
    "                    batch_size = 20,\n",
    "                    class_mode = 'binary')\n",
    "y_pred = []\n",
    "y_label = []\n",
    "for i in range(50):\n",
    "    x,y = test_generator.next()\n",
    "    y = y.ravel()\n",
    "    temp_y = model.predict(x)\n",
    "    temp_y = np.round(temp_y.ravel())\n",
    "    misclassification = np.absolute(y-temp_y)\n",
    "\n",
    "    #show the misclassification\n",
    "    misclass_index = np.where(misclassification > 0.5)\n",
    "    for j in range(len(misclass_index[0])):\n",
    "        plt.figure()\n",
    "        #print(misclass_index[0][j])\n",
    "        plt.title('label is {}, prediction is {}'.format(y[misclass_index[0][j]],temp_y[misclass_index[0][j]]))\n",
    "        plt.imshow(image.array_to_img(x[misclass_index[0][j]]))\n",
    "    misclass_index = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#to see how the generator encodes the images into binary or multiclass:\n",
    "test_generator.class_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_generator.classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#another grand model with just relu\n",
    "from keras.layers import LeakyReLU\n",
    "from keras.regularizers import l2\n",
    "model = models.Sequential()\n",
    "model.add(layers.Conv2D(32,(3,3), activation = 'relu', input_shape = (150,150,3)))\n",
    "# model.add(LeakyReLU(alpha=0.1))\n",
    "model.add(layers.MaxPooling2D((2,2)))\n",
    "model.add(layers.Dropout(0.25))\n",
    "\n",
    "model.add(layers.Conv2D(64,(3,3), activation = 'relu'))\n",
    "#model.add(LeakyReLU(alpha=0.1))\n",
    "model.add(layers.MaxPooling2D((2,2)))\n",
    "model.add(layers.Dropout(0.25))\n",
    "\n",
    "model.add(layers.Conv2D(128,(3,3), activation = 'relu'))\n",
    "#model.add(LeakyReLU(alpha=0.1))\n",
    "model.add(layers.MaxPooling2D((2,2)))\n",
    "model.add(layers.Dropout(0.25))\n",
    "\n",
    "model.add(layers.Conv2D(128,(3,3), activation = 'relu'))\n",
    "#model.add(LeakyReLU(alpha=0.1))\n",
    "model.add(layers.MaxPooling2D((2,2)))\n",
    "model.add(layers.Dropout(0.25))\n",
    "\n",
    "model.add(layers.Flatten())\n",
    "model.add(layers.Dropout(0.5))\n",
    "model.add(layers.Dense(512, activation = 'relu',W_regularizer = l2(1e-4)))\n",
    "#model.add(LeakyReLU(alpha=0.1))\n",
    "model.add(layers.Dense(1, activation = 'sigmoid'))\n",
    "#compile the model with RMSprob with learning rate\n",
    "from keras import optimizers\n",
    "model.compile(loss = 'binary_crossentropy', optimizer = optimizers.RMSprop(lr=1e-4), metrics = ['acc'])\n",
    "\n",
    "#process the jpeg image\n",
    "#create an image generator\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "#train using data augmentation and dropout\n",
    "train_datagen = ImageDataGenerator(\n",
    "                    rescale = 1./255,\n",
    "                    rotation_range = 40,\n",
    "                    width_shift_range = 0.2,\n",
    "                    height_shift_range = 0.2,\n",
    "                    shear_range = 0.2,\n",
    "                    zoom_range = 0.2,\n",
    "                    horizontal_flip = True)\n",
    "test_datagen = ImageDataGenerator(rescale = 1./255) #validation data should not be augmented\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "                    '/home/ubuntu/train', \n",
    "                    target_size = (150,150),\n",
    "                    batch_size = 32,\n",
    "                    class_mode = 'binary')\n",
    "validation_generator = test_datagen.flow_from_directory(\n",
    "                    '/home/ubuntu/validation', \n",
    "                    target_size = (150,150),\n",
    "                    batch_size = 32,\n",
    "                    class_mode = 'binary')\n",
    "\n",
    "#fit the model from image generator\n",
    "history = model.fit_generator(\n",
    "            train_generator,\n",
    "            steps_per_epoch=100,\n",
    "            epochs=100,\n",
    "            validation_data=validation_generator,\n",
    "            validation_steps=50)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#save the champion model (leaky, full dropout, L2 regularization, data aug)\n",
    "model.save('tomnod_everything_relu_Adam.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot loss and accuracy for detection overfitting\n",
    "import matplotlib.pyplot as plt\n",
    "acc = history.history['acc']\n",
    "val_acc = history.history['val_acc']\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "epochs = range(1,len(acc)+1)\n",
    "plt.plot(epochs, acc, 'bo', label = 'Training acc')\n",
    "plt.plot(epochs, val_acc, 'r', label = 'Validation acc')\n",
    "plt.title('Train and validation accuracy with Adam, ReLU, full dropout, L2 regularization, data aug')\n",
    "plt.legend()\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(epochs, loss, 'bo', label = 'Training loss')\n",
    "plt.plot(epochs, val_loss, 'r', label = 'Validation loss')\n",
    "plt.title('Train and validation loss with Adam, ReLU, full dropout, L2 regularization, data aug')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = load_model('tomnod_2_100epochs_Adam.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "128*128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
